"""
ä¸“é—¨ç”¨äºAè‚¡è´¢æŠ¥æ•°æ®è·å–çš„å·¥å…·
æä¾›ç¨³å®šã€å¯é çš„è´¢åŠ¡æ•°æ®æ¥å£ï¼Œé¿å…æ™ºèƒ½ä½“ç”Ÿæˆä»£ç çš„é”™è¯¯
åŒ…å«æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union
from datetime import datetime, timedelta
import logging
import traceback
import json
import os
import pickle
from pathlib import Path
import hashlib

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FinancialDataCache:
    """è´¢åŠ¡æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None):
        if cache_dir is None:
            # é»˜è®¤ç¼“å­˜ç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
            project_root = Path(__file__).parent.parent
            cache_dir = project_root / "financial_data_cache"
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()
        
        logger.info(f"è´¢åŠ¡æ•°æ®ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {
            "stocks": {},
            "last_cleanup": datetime.now().isoformat(),
            "version": "1.0"
        }
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_cache_key(self, stock_code: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"stock_{stock_code}"
    
    def get_cache_file_path(self, stock_code: str, data_type: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self.get_cache_key(stock_code)
        return self.cache_dir / f"{cache_key}_{data_type}.pkl"
    
    def is_data_cached(self, stock_code: str) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç¼“å­˜"""
        cache_key = self.get_cache_key(stock_code)
        
        if cache_key not in self.metadata["stocks"]:
            return False
        
        # æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_types = ["income", "balance", "cashflow", "metrics", "trend"]
        for data_type in data_types:
            cache_file = self.get_cache_file_path(stock_code, data_type)
            if not cache_file.exists():
                return False
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆé»˜è®¤7å¤©ï¼‰
        cache_info = self.metadata["stocks"][cache_key]
        cache_date = datetime.fromisoformat(cache_info["cached_date"])
        if datetime.now() - cache_date > timedelta(days=7):
            logger.info(f"ç¼“å­˜æ•°æ®å·²è¿‡æœŸ: {stock_code}")
            return False
        
        return True
    
    def get_cached_data(self, stock_code: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æ•°æ®"""
        if not self.is_data_cached(stock_code):
            return None
        
        try:
            data = {}
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            
            for data_type in data_types:
                cache_file = self.get_cache_file_path(stock_code, data_type)
                if cache_file.exists():
                    with open(cache_file, 'rb') as f:
                        data[data_type] = pickle.load(f)
            
            cache_key = self.get_cache_key(stock_code)
            cache_info = self.metadata["stocks"][cache_key]
            logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {stock_code} (ç¼“å­˜æ—¶é—´: {cache_info['cached_date']})")
            
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def cache_data(self, stock_code: str, data: Dict[str, pd.DataFrame], 
                   latest_report_date: str = None):
        """ç¼“å­˜æ•°æ®"""
        try:
            cache_key = self.get_cache_key(stock_code)
            
            # ä¿å­˜å„ç±»æ•°æ®
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            saved_count = 0
            
            for data_type in data_types:
                if data_type in data and data[data_type] is not None:
                    cache_file = self.get_cache_file_path(stock_code, data_type)
                    with open(cache_file, 'wb') as f:
                        pickle.dump(data[data_type], f)
                    saved_count += 1
            
            # æ›´æ–°å…ƒæ•°æ®
            self.metadata["stocks"][cache_key] = {
                "stock_code": stock_code,
                "cached_date": datetime.now().isoformat(),
                "latest_report_date": latest_report_date,
                "data_types": [dt for dt in data_types if dt in data and data[dt] is not None],
                "file_count": saved_count
            }
            
            self._save_metadata()
            logger.info(f"æ•°æ®ç¼“å­˜æˆåŠŸ: {stock_code} (ä¿å­˜äº†{saved_count}ä¸ªæ–‡ä»¶)")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    def needs_update(self, stock_code: str, current_data: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°"""
        if not self.is_data_cached(stock_code):
            return True
        
        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_key = self.get_cache_key(stock_code)
        cache_info = self.metadata["stocks"][cache_key]
        
        # å¦‚æœå½“å‰æ•°æ®æœ‰æ›´æ–°çš„æŠ¥å‘Šæ—¥æœŸï¼Œåˆ™éœ€è¦æ›´æ–°
        if "income" in current_data and not current_data["income"].empty:
            latest_cached_date = cache_info.get("latest_report_date", "")
            
            # è·å–å½“å‰æ•°æ®çš„æœ€æ–°æŠ¥å‘Šæ—¥æœŸ
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in current_data["income"].columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in current_data["income"].columns:
                current_latest_date = pd.to_datetime(current_data["income"][date_col].iloc[0]).strftime('%Y-%m-%d')
                
                if current_latest_date > latest_cached_date:
                    logger.info(f"æ£€æµ‹åˆ°æ–°è´¢æŠ¥æ•°æ®: {stock_code} {latest_cached_date} -> {current_latest_date}")
                    return True
        
        return False
    
    def cleanup_old_cache(self, days: int = 30):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            cleaned_count = 0
            
            # æ¸…ç†è¿‡æœŸçš„è‚¡ç¥¨æ•°æ®
            expired_stocks = []
            for cache_key, cache_info in self.metadata["stocks"].items():
                cache_date = datetime.fromisoformat(cache_info["cached_date"])
                if cache_date < cutoff_date:
                    expired_stocks.append(cache_key)
            
            for cache_key in expired_stocks:
                # åˆ é™¤æ•°æ®æ–‡ä»¶
                data_types = ["income", "balance", "cashflow", "metrics", "trend"]
                for data_type in data_types:
                    cache_file = self.get_cache_file_path(cache_key.replace("stock_", ""), data_type)
                    if cache_file.exists():
                        cache_file.unlink()
                
                # ä»å…ƒæ•°æ®ä¸­åˆ é™¤
                del self.metadata["stocks"][cache_key]
                cleaned_count += 1
            
            if cleaned_count > 0:
                self._save_metadata()
                logger.info(f"æ¸…ç†äº†{cleaned_count}ä¸ªè¿‡æœŸç¼“å­˜")
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")


class AKShareFinancialDataTool:
    """AKShareè´¢åŠ¡æ•°æ®è·å–ä¸“ç”¨å·¥å…·ï¼ˆå¸¦æ™ºèƒ½ç¼“å­˜ï¼‰"""
    
    def __init__(self, cache_dir: str = None):
        self.akshare = None
        self.cache = FinancialDataCache(cache_dir)
        self._setup_akshare()
        
    def _setup_akshare(self):
        """è®¾ç½®AKShare"""
        try:
            import akshare as ak
            self.akshare = ak
            logger.info("AKShareåˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.error("AKShareæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "akshare>=1.12.0"])
            import akshare as ak
            self.akshare = ak
            logger.info("AKShareå®‰è£…æˆåŠŸ")
        except Exception as e:
            logger.error(f"AKShareè®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _convert_stock_code(self, stock_code: str) -> str:
        """è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼"""
        if stock_code.startswith('6'):
            return f"SH{stock_code}"
        elif stock_code.startswith('0') or stock_code.startswith('3'):
            return f"SZ{stock_code}"
        else:
            return stock_code
    
    def _clean_financial_data(self, df: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """æ¸…æ´—è´¢åŠ¡æ•°æ®"""
        try:
            # åˆ é™¤ç©ºå€¼è¡Œ
            if 'REPORT_DATE' in df.columns:
                df = df[df['REPORT_DATE'].notna()].copy()
            elif 'æŠ¥å‘ŠæœŸ' in df.columns:
                df = df[df['æŠ¥å‘ŠæœŸ'].notna()].copy()
            
            # è½¬æ¢æ•°å€¼åˆ—
            exclude_cols = ['REPORT_DATE', 'æŠ¥å‘ŠæœŸ', 'æŠ¥è¡¨ç±»å‹', 'REPORT_TYPE', 
                          'SECUCODE', 'SECURITY_CODE', 'SECURITY_NAME_ABBR']
            
            for col in df.columns:
                if col not in exclude_cols:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # è½¬æ¢æ—¥æœŸåˆ—å¹¶æ’åº
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in df.columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in df.columns:
                df[date_col] = pd.to_datetime(df[date_col])
                df = df.sort_values(date_col, ascending=False)
            
            logger.info(f"{data_type}æ•°æ®æ¸…æ´—å®Œæˆï¼Œå‰©ä½™{len(df)}è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"{data_type}æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            return df
    
    def get_financial_reports(self, stock_code: str, stock_name: str = None, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
        """
        è·å–å®Œæ•´è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼ˆå¸¦æ™ºèƒ½ç¼“å­˜ï¼‰
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '600248'
            stock_name: è‚¡ç¥¨åç§°ï¼Œå¯é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            åŒ…å«åˆ©æ¶¦è¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨çš„å­—å…¸
        """
        if stock_name is None:
            stock_name = f"è‚¡ç¥¨{stock_code}"
            
        logger.info(f"å¼€å§‹è·å–{stock_name}({stock_code})è´¢åŠ¡æ•°æ®")
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh:
            cached_data = self.cache.get_cached_data(stock_code)
            if cached_data:
                logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {stock_code}")
                return cached_data
        
        symbol = self._convert_stock_code(stock_code)
        logger.info(f"è‚¡ç¥¨ä»£ç è½¬æ¢: {stock_code} -> {symbol}")
        
        results = {}
        
        try:
            # 1. è·å–åˆ©æ¶¦è¡¨
            logger.info("è·å–åˆ©æ¶¦è¡¨...")
            try:
                income_df = self.akshare.stock_profit_sheet_by_report_em(symbol=symbol)
                income_df = self._clean_financial_data(income_df, "åˆ©æ¶¦è¡¨")
                results['income'] = income_df
                logger.info(f"åˆ©æ¶¦è¡¨è·å–æˆåŠŸ: {len(income_df)}è¡Œ")
            except Exception as e:
                logger.error(f"åˆ©æ¶¦è¡¨è·å–å¤±è´¥: {e}")
                # å°è¯•å¤‡ç”¨æ–¹æ³•
                try:
                    income_df = self.akshare.stock_lrb_em(symbol=stock_code)
                    income_df = self._clean_financial_data(income_df, "åˆ©æ¶¦è¡¨(å¤‡ç”¨)")
                    results['income'] = income_df
                    logger.info(f"åˆ©æ¶¦è¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(income_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"åˆ©æ¶¦è¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # 2. è·å–èµ„äº§è´Ÿå€ºè¡¨
            logger.info("è·å–èµ„äº§è´Ÿå€ºè¡¨...")
            try:
                balance_df = self.akshare.stock_balance_sheet_by_report_em(symbol=symbol)
                balance_df = self._clean_financial_data(balance_df, "èµ„äº§è´Ÿå€ºè¡¨")
                results['balance'] = balance_df
                logger.info(f"èµ„äº§è´Ÿå€ºè¡¨è·å–æˆåŠŸ: {len(balance_df)}è¡Œ")
            except Exception as e:
                logger.error(f"èµ„äº§è´Ÿå€ºè¡¨è·å–å¤±è´¥: {e}")
                try:
                    balance_df = self.akshare.stock_zcfz_em(symbol=stock_code)
                    balance_df = self._clean_financial_data(balance_df, "èµ„äº§è´Ÿå€ºè¡¨(å¤‡ç”¨)")
                    results['balance'] = balance_df
                    logger.info(f"èµ„äº§è´Ÿå€ºè¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(balance_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"èµ„äº§è´Ÿå€ºè¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # 3. è·å–ç°é‡‘æµé‡è¡¨
            logger.info("è·å–ç°é‡‘æµé‡è¡¨...")
            try:
                cashflow_df = self.akshare.stock_cash_flow_sheet_by_report_em(symbol=symbol)
                cashflow_df = self._clean_financial_data(cashflow_df, "ç°é‡‘æµé‡è¡¨")
                results['cashflow'] = cashflow_df
                logger.info(f"ç°é‡‘æµé‡è¡¨è·å–æˆåŠŸ: {len(cashflow_df)}è¡Œ")
            except Exception as e:
                logger.error(f"ç°é‡‘æµé‡è¡¨è·å–å¤±è´¥: {e}")
                try:
                    cashflow_df = self.akshare.stock_xjll_em(symbol=stock_code)
                    cashflow_df = self._clean_financial_data(cashflow_df, "ç°é‡‘æµé‡è¡¨(å¤‡ç”¨)")
                    results['cashflow'] = cashflow_df
                    logger.info(f"ç°é‡‘æµé‡è¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(cashflow_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"ç°é‡‘æµé‡è¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜
            if not force_refresh and self.cache.needs_update(stock_code, results):
                logger.info(f"æ£€æµ‹åˆ°æ–°æ•°æ®ï¼Œæ›´æ–°ç¼“å­˜: {stock_code}")
            
            # è®¡ç®—å¹¶ç¼“å­˜æŒ‡æ ‡å’Œè¶‹åŠ¿æ•°æ®
            if results:
                metrics = self.get_key_metrics(results)
                trend = self.get_historical_trend(results)
                
                results['metrics'] = pd.DataFrame([metrics]) if metrics else pd.DataFrame()
                results['trend'] = trend
                
                # è·å–æœ€æ–°æŠ¥å‘Šæ—¥æœŸ
                latest_report_date = ""
                if 'income' in results and not results['income'].empty:
                    date_col = 'REPORT_DATE' if 'REPORT_DATE' in results['income'].columns else 'æŠ¥å‘ŠæœŸ'
                    if date_col in results['income'].columns:
                        latest_report_date = pd.to_datetime(results['income'][date_col].iloc[0]).strftime('%Y-%m-%d')
                
                # ç¼“å­˜æ•°æ®
                self.cache.cache_data(stock_code, results, latest_report_date)
                logger.info(f"æ•°æ®å·²ç¼“å­˜: {stock_code}")
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–è´¢åŠ¡æŠ¥è¡¨å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return results
    
    def get_key_metrics(self, financial_data: Dict[str, pd.DataFrame]) -> Dict[str, float]:
        """
        æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            
        Returns:
            å…³é”®è´¢åŠ¡æŒ‡æ ‡å­—å…¸
        """
        logger.info("æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡")
        
        metrics = {}
        
        try:
            if not financial_data:
                logger.error("è´¢åŠ¡æ•°æ®ä¸ºç©º")
                return metrics
                
            # è·å–æœ€æ–°ä¸€æœŸæ•°æ®
            latest_income = financial_data.get('income', pd.DataFrame())
            latest_balance = financial_data.get('balance', pd.DataFrame())
            latest_cashflow = financial_data.get('cashflow', pd.DataFrame())
            
            if latest_income.empty:
                logger.error("åˆ©æ¶¦è¡¨æ•°æ®ä¸ºç©º")
                return metrics
                
            latest_income = latest_income.iloc[0]
            
            # æå–ç›ˆåˆ©èƒ½åŠ›æŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
            revenue_cols = ['TOTAL_OPERATE_INCOME', 'è¥ä¸šæ”¶å…¥', 'operating_revenue']
            net_profit_cols = ['NETPROFIT', 'å‡€åˆ©æ¶¦', 'net_profit']
            parent_profit_cols = ['PARENT_NETPROFIT', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦', 'parent_net_profit']
            
            revenue = self._get_value_by_col_names(latest_income, revenue_cols) / 1e8
            net_profit = self._get_value_by_col_names(latest_income, net_profit_cols) / 1e8
            parent_profit = self._get_value_by_col_names(latest_income, parent_profit_cols) / 1e8
            
            metrics['revenue_billion'] = revenue
            metrics['net_profit_billion'] = net_profit
            metrics['parent_profit_billion'] = parent_profit
            metrics['net_profit_margin'] = (net_profit / revenue * 100) if revenue > 0 else 0
            
            # æå–è´¢åŠ¡çŠ¶å†µæŒ‡æ ‡
            if not latest_balance.empty:
                latest_balance = latest_balance.iloc[0]
                
                asset_cols = ['TOTAL_ASSETS', 'èµ„äº§æ€»è®¡', 'total_assets']
                liability_cols = ['TOTAL_LIABILITIES', 'è´Ÿå€ºåˆè®¡', 'total_liabilities']
                equity_cols = ['TOTAL_EQUITY', 'æ‰€æœ‰è€…æƒç›Šåˆè®¡', 'total_equity']
                
                total_assets = self._get_value_by_col_names(latest_balance, asset_cols) / 1e8
                total_liabilities = self._get_value_by_col_names(latest_balance, liability_cols) / 1e8
                total_equity = self._get_value_by_col_names(latest_balance, equity_cols) / 1e8
                
                metrics['total_assets_billion'] = total_assets
                metrics['total_liabilities_billion'] = total_liabilities
                metrics['total_equity_billion'] = total_equity
                metrics['debt_to_asset_ratio'] = (total_liabilities / total_assets * 100) if total_assets > 0 else 0
                
                # è®¡ç®—ROE
                if total_equity > 0:
                    metrics['roe'] = parent_profit / total_equity * 100
            
            logger.info("å…³é”®æŒ‡æ ‡æå–å®Œæˆ")
            return metrics
            
        except Exception as e:
            logger.error(f"æå–å…³é”®æŒ‡æ ‡å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return metrics
    
    def _get_value_by_col_names(self, row: pd.Series, col_names: List[str]) -> float:
        """æ ¹æ®å¯èƒ½çš„åˆ—åè·å–æ•°å€¼"""
        for col in col_names:
            if col in row.index and pd.notna(row[col]):
                return float(row[col])
        return 0.0
    
    def get_historical_trend(self, financial_data: Dict[str, pd.DataFrame], years: int = 4) -> pd.DataFrame:
        """
        è·å–å†å²è¶‹åŠ¿æ•°æ®
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            years: è·å–å¹´æ•°
            
        Returns:
            è¶‹åŠ¿æ•°æ®DataFrame
        """
        logger.info(f"è·å–æœ€è¿‘{years}å¹´è¶‹åŠ¿æ•°æ®")
        
        try:
            income_df = financial_data.get('income', pd.DataFrame())
            if income_df.empty:
                logger.error("åˆ©æ¶¦è¡¨æ•°æ®ä¸ºç©º")
                return pd.DataFrame()
            
            # è·å–æœ€è¿‘Nå¹´æ•°æ®
            trend_data = income_df.head(years).copy()
            
            # ç¡®å®šæ—¥æœŸåˆ—å
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in trend_data.columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in trend_data.columns:
                trend_data['å¹´ä»½'] = pd.to_datetime(trend_data[date_col]).dt.year
            else:
                logger.error("æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
                return pd.DataFrame()
            
            # æå–å…³é”®æŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
            revenue_cols = ['TOTAL_OPERATE_INCOME', 'è¥ä¸šæ”¶å…¥', 'operating_revenue']
            profit_cols = ['NETPROFIT', 'å‡€åˆ©æ¶¦', 'net_profit']
            
            result_df = pd.DataFrame()
            result_df['å¹´ä»½'] = trend_data['å¹´ä»½']
            result_df['è¥ä¸šæ”¶å…¥'] = self._get_values_by_col_names(trend_data, revenue_cols) / 1e8
            result_df['å‡€åˆ©æ¶¦'] = self._get_values_by_col_names(trend_data, profit_cols) / 1e8
            
            logger.info(f"è¶‹åŠ¿æ•°æ®è·å–æˆåŠŸ: {len(result_df)}å¹´")
            return result_df
            
        except Exception as e:
            logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _get_values_by_col_names(self, df: pd.DataFrame, col_names: List[str]) -> pd.Series:
        """æ ¹æ®å¯èƒ½çš„åˆ—åè·å–æ•°å€¼åˆ—"""
        for col in col_names:
            if col in df.columns:
                return df[col]
        return pd.Series([0.0] * len(df), index=df.index)
    
    def save_to_csv(self, financial_data: Dict[str, pd.DataFrame], filepath_prefix: str):
        """
        ä¿å­˜è´¢åŠ¡æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            filepath_prefix: æ–‡ä»¶è·¯å¾„å‰ç¼€
        """
        logger.info(f"ä¿å­˜è´¢åŠ¡æ•°æ®åˆ° {filepath_prefix}")
        
        try:
            for data_type, df in financial_data.items():
                if not df.empty:
                    filepath = f"{filepath_prefix}_{data_type}.csv"
                    df.to_csv(filepath, index=False, encoding='utf-8-sig')
                    logger.info(f"{data_type}æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                    
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    def check_cache_status(self, stock_code: str) -> Dict:
        """
        æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        """
        cache_key = self.cache.get_cache_key(stock_code)
        
        if cache_key in self.cache.metadata["stocks"]:
            cache_info = self.cache.metadata["stocks"][cache_key]
            return {
                "cached": True,
                "cached_date": cache_info["cached_date"],
                "latest_report_date": cache_info.get("latest_report_date", ""),
                "data_types": cache_info.get("data_types", []),
                "file_count": cache_info.get("file_count", 0)
            }
        else:
            return {
                "cached": False,
                "cached_date": "",
                "latest_report_date": "",
                "data_types": [],
                "file_count": 0
            }
    
    def refresh_cache(self, stock_code: str, stock_name: str = None) -> bool:
        """
        å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"å¼ºåˆ¶åˆ·æ–°ç¼“å­˜: {stock_code}")
        
        try:
            # å¼ºåˆ¶è·å–æ–°æ•°æ®
            new_data = self.get_financial_reports(stock_code, stock_name, force_refresh=True)
            
            if new_data and ('income' in new_data and not new_data['income'].empty):
                logger.info(f"ç¼“å­˜åˆ·æ–°æˆåŠŸ: {stock_code}")
                return True
            else:
                logger.warning(f"ç¼“å­˜åˆ·æ–°å¤±è´¥ï¼Œæ•°æ®ä¸ºç©º: {stock_code}")
                return False
                
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ·æ–°å¤±è´¥: {e}")
            return False
    
    def cleanup_cache(self, days: int = 30):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        logger.info(f"æ¸…ç†{days}å¤©å‰çš„ç¼“å­˜")
        self.cache.cleanup_old_cache(days)
    
    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜æ•´ä½“ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        stocks = self.cache.metadata["stocks"]
        total_files = sum(info.get("file_count", 0) for info in stocks.values())
        
        # è®¡ç®—ç¼“å­˜å¤§å°
        total_size = 0
        for stock_code in stocks.keys():
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            for data_type in data_types:
                cache_file = self.cache.get_cache_file_path(stock_code.replace("stock_", ""), data_type)
                if cache_file.exists():
                    total_size += cache_file.stat().st_size
        
        return {
            "total_stocks": len(stocks),
            "total_files": total_files,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "last_cleanup": self.cache.metadata.get("last_cleanup", ""),
            "cache_directory": str(self.cache.cache_dir)
        }
    
    def clear_all_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        logger.warning("æ¸…é™¤æ‰€æœ‰ç¼“å­˜æ•°æ®")
        
        try:
            # åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            for stock_info in list(self.cache.metadata["stocks"].values()):
                stock_code = stock_info["stock_code"]
                data_types = ["income", "balance", "cashflow", "metrics", "trend"]
                for data_type in data_types:
                    cache_file = self.cache.get_cache_file_path(stock_code, data_type)
                    if cache_file.exists():
                        cache_file.unlink()
            
            # é‡ç½®å…ƒæ•°æ®
            self.cache.metadata = {
                "stocks": {},
                "last_cleanup": datetime.now().isoformat(),
                "version": "1.0"
            }
            self.cache._save_metadata()
            
            logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")
            
        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def export_cache_summary(self, filepath: str = None):
        """
        å¯¼å‡ºç¼“å­˜æ‘˜è¦
        
        Args:
            filepath: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if filepath is None:
            filepath = self.cache.cache_dir / "cache_summary.csv"
        
        try:
            summary_data = []
            for cache_key, info in self.cache.metadata["stocks"].items():
                summary_data.append({
                    "stock_code": info["stock_code"],
                    "cached_date": info["cached_date"],
                    "latest_report_date": info.get("latest_report_date", ""),
                    "data_types": ", ".join(info.get("data_types", [])),
                    "file_count": info.get("file_count", 0)
                })
            
            df = pd.DataFrame(summary_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            logger.info(f"ç¼“å­˜æ‘˜è¦å·²å¯¼å‡ºåˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")


# åˆ›å»ºå•ä¾‹å®ä¾‹
_financial_tool = None

def get_financial_tool(cache_dir: str = None):
    """è·å–è´¢åŠ¡å·¥å…·å•ä¾‹"""
    global _financial_tool
    if _financial_tool is None:
        _financial_tool = AKShareFinancialDataTool(cache_dir)
    return _financial_tool


# ä¾¿åˆ©å‡½æ•°
def get_financial_reports(stock_code: str, stock_name: str = None, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
    """è·å–è´¢åŠ¡æŠ¥è¡¨æ•°æ®çš„ä¾¿åˆ©å‡½æ•°"""
    tool = get_financial_tool()
    return tool.get_financial_reports(stock_code, stock_name, force_refresh)

def get_key_metrics(financial_data: Dict[str, pd.DataFrame]) -> Dict[str, float]:
    """è·å–å…³é”®è´¢åŠ¡æŒ‡æ ‡çš„ä¾¿åˆ©å‡½æ•°"""
    tool = get_financial_tool()
    return tool.get_key_metrics(financial_data)

def get_historical_trend(financial_data: Dict[str, pd.DataFrame], years: int = 4) -> pd.DataFrame:
    """è·å–å†å²è¶‹åŠ¿æ•°æ®çš„ä¾¿åˆ©å‡½æ•°"""
    tool = get_financial_tool()
    return tool.get_historical_trend(financial_data, years)

# ç¼“å­˜ç®¡ç†ä¾¿åˆ©å‡½æ•°
def check_cache_status(stock_code: str) -> Dict:
    """æ£€æŸ¥ç¼“å­˜çŠ¶æ€"""
    tool = get_financial_tool()
    return tool.check_cache_status(stock_code)

def refresh_cache(stock_code: str, stock_name: str = None) -> bool:
    """åˆ·æ–°ç¼“å­˜"""
    tool = get_financial_tool()
    return tool.refresh_cache(stock_code, stock_name)

def cleanup_cache(days: int = 30):
    """æ¸…ç†è¿‡æœŸç¼“å­˜"""
    tool = get_financial_tool()
    tool.cleanup_cache(days)

def get_cache_info() -> Dict:
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    tool = get_financial_tool()
    return tool.get_cache_info()

def clear_all_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
    tool = get_financial_tool()
    tool.clear_all_cache()

def export_cache_summary(filepath: str = None):
    """å¯¼å‡ºç¼“å­˜æ‘˜è¦"""
    tool = get_financial_tool()
    tool.export_cache_summary(filepath)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== AKShareè´¢åŠ¡æ•°æ®å·¥å…·å®Œæ•´æµ‹è¯•ï¼ˆå«æ™ºèƒ½ç¼“å­˜ï¼‰ ===\n")
    
    # åˆå§‹åŒ–å·¥å…·
    tool = AKShareFinancialDataTool()
    
    # æµ‹è¯•1: é¦–æ¬¡è·å–æ•°æ®ï¼ˆåº”è¯¥ä»ç½‘ç»œè·å–ï¼‰
    print("1. é¦–æ¬¡è·å–é™•è¥¿å»ºå·¥æ•°æ®ï¼ˆä»ç½‘ç»œè·å–ï¼‰...")
    data1 = tool.get_financial_reports("600248", "é™•è¥¿å»ºå·¥")
    
    if data1:
        print("   âœ“ æ•°æ®è·å–æˆåŠŸ")
        
        # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        cache_status = tool.check_cache_status("600248")
        print(f"   âœ“ ç¼“å­˜çŠ¶æ€: {cache_status}")
        
        # æµ‹è¯•å…³é”®æŒ‡æ ‡
        metrics = tool.get_key_metrics(data1)
        if metrics:
            print(f"   âœ“ å…³é”®æŒ‡æ ‡: è¥æ”¶ {metrics.get('revenue_billion', 0):.1f}äº¿å…ƒ")
    
    # æµ‹è¯•2: ç¬¬äºŒæ¬¡è·å–æ•°æ®ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
    print("\n2. ç¬¬äºŒæ¬¡è·å–é™•è¥¿å»ºå·¥æ•°æ®ï¼ˆä»ç¼“å­˜è·å–ï¼‰...")
    data2 = tool.get_financial_reports("600248", "é™•è¥¿å»ºå·¥")
    
    if data2:
        print("   âœ“ ç¼“å­˜æ•°æ®è·å–æˆåŠŸ")
        
        # æ¯”è¾ƒä¸¤æ¬¡æ•°æ®æ˜¯å¦ä¸€è‡´
        if data1 and data2:
            income_match = len(data1.get('income', pd.DataFrame())) == len(data2.get('income', pd.DataFrame()))
            print(f"   âœ“ æ•°æ®ä¸€è‡´æ€§: {'é€šè¿‡' if income_match else 'å¤±è´¥'}")
    
    # æµ‹è¯•3: ç¼“å­˜ç®¡ç†åŠŸèƒ½
    print("\n3. æµ‹è¯•ç¼“å­˜ç®¡ç†åŠŸèƒ½...")
    
    # è·å–ç¼“å­˜ä¿¡æ¯
    cache_info = tool.get_cache_info()
    print(f"   âœ“ ç¼“å­˜ç»Ÿè®¡: {cache_info['total_stocks']}åªè‚¡ç¥¨, {cache_info['total_size_mb']}MB")
    
    # æµ‹è¯•å¼ºåˆ¶åˆ·æ–°
    refresh_success = tool.refresh_cache("600248", "é™•è¥¿å»ºå·¥")
    print(f"   âœ“ å¼ºåˆ¶åˆ·æ–°: {'æˆåŠŸ' if refresh_success else 'å¤±è´¥'}")
    
    # æµ‹è¯•4: å¤šè‚¡ç¥¨ç¼“å­˜
    print("\n4. æµ‹è¯•å¤šè‚¡ç¥¨ç¼“å­˜...")
    test_stocks = [("000858", "äº”ç²®æ¶²"), ("600519", "è´µå·èŒ…å°")]
    
    for code, name in test_stocks:
        print(f"   è·å– {name}({code})...")
        try:
            data = tool.get_financial_reports(code, name)
            if data and 'income' in data and not data['income'].empty:
                metrics = tool.get_key_metrics(data)
                print(f"     âœ“ æˆåŠŸ - è¥æ”¶: {metrics.get('revenue_billion', 0):.1f}äº¿å…ƒ")
            else:
                print(f"     âœ— å¤±è´¥")
        except Exception as e:
            print(f"     âœ— å¼‚å¸¸: {e}")
    
    # æµ‹è¯•5: æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
    print("\n5. æœ€ç»ˆç¼“å­˜ç»Ÿè®¡...")
    final_cache_info = tool.get_cache_info()
    print(f"   æ€»ç¼“å­˜è‚¡ç¥¨æ•°: {final_cache_info['total_stocks']}")
    print(f"   æ€»ç¼“å­˜æ–‡ä»¶æ•°: {final_cache_info['total_files']}")
    print(f"   æ€»ç¼“å­˜å¤§å°: {final_cache_info['total_size_mb']}MB")
    print(f"   ç¼“å­˜ç›®å½•: {final_cache_info['cache_directory']}")
    
    # å¯¼å‡ºç¼“å­˜æ‘˜è¦
    tool.export_cache_summary()
    print("   âœ“ ç¼“å­˜æ‘˜è¦å·²å¯¼å‡º")
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print("âœ“ æ™ºèƒ½ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
    print("âœ“ å¢é‡æ›´æ–°æœºåˆ¶å·¥ä½œ")
    print("âœ“ ç¼“å­˜ç®¡ç†åŠŸèƒ½å®Œæ•´")
    print("âœ“ æ•°æ®ä¸€è‡´æ€§ä¿è¯")
    print("\nğŸ‰ å·¥å…·å‡çº§å®Œæˆï¼ç°åœ¨å…·å¤‡æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°åŠŸèƒ½ã€‚")
    print("\nä¸»è¦ç‰¹æ€§:")
    print("- åŒä¸€å…¬å¸æ•°æ®è·å–ä¸€æ¬¡åè‡ªåŠ¨ç¼“å­˜")
    print("- æ£€æµ‹æ–°è´¢æŠ¥æ•°æ®å¹¶è‡ªåŠ¨å¢é‡æ›´æ–°")
    print("- ç¼“å­˜æœ‰æ•ˆæœŸç®¡ç†ï¼ˆé»˜è®¤7å¤©ï¼‰")
    print("- è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜")
    print("- å®Œæ•´çš„ç¼“å­˜ç®¡ç†åŠŸèƒ½")