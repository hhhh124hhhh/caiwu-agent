"""
ä¸“é—¨ç”¨äºAè‚¡è´¢æŠ¥æ•°æ®è·å–çš„å·¥å…·åŒ…ç±»
æä¾›ç¨³å®šã€å¯é çš„è´¢åŠ¡æ•°æ®æ¥å£ï¼Œé¿å…æ™ºèƒ½ä½“ç”Ÿæˆä»£ç çš„é”™è¯¯
åŒ…å«æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°åŠŸèƒ½
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Union, Any
from datetime import datetime, timedelta
import logging
import traceback
import json
import os
import pickle
from pathlib import Path
import hashlib

from ..config import ToolkitConfig
from .base import AsyncBaseToolkit, register_tool

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FinancialDataCache:
    """è´¢åŠ¡æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        if cache_dir is None:
            # é»˜è®¤ç¼“å­˜ç›®å½•åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹
            project_root = Path(__file__).parent.parent
            cache_dir = str(project_root / "financial_data_cache")
        
        # ç¡®ä¿cache_diræ˜¯å­—ç¬¦ä¸²ç±»å‹
        cache_dir_str: str = cache_dir if cache_dir is not None else ""
        self.cache_dir = Path(cache_dir_str)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # å…ƒæ•°æ®æ–‡ä»¶
        self.metadata_file = self.cache_dir / "cache_metadata.json"
        self.metadata = self._load_metadata()
        
        logger.info(f"è´¢åŠ¡æ•°æ®ç¼“å­˜åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½ç¼“å­˜å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
        
        return {
            "stocks": {},
            "last_cleanup": datetime.now().isoformat(),
            "version": "1.0"
        }
    
    def _save_metadata(self):
        """ä¿å­˜ç¼“å­˜å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
    
    def get_cache_key(self, stock_code: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"stock_{stock_code}"
    
    def get_cache_file_path(self, stock_code: str, data_type: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        cache_key = self.get_cache_key(stock_code)
        return self.cache_dir / f"{cache_key}_{data_type}.pkl"
    
    def is_data_cached(self, stock_code: str) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦å·²ç¼“å­˜"""
        cache_key = self.get_cache_key(stock_code)
        
        if cache_key not in self.metadata["stocks"]:
            return False
        
        # æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        data_types = ["income", "balance", "cashflow", "metrics", "trend"]
        for data_type in data_types:
            cache_file = self.get_cache_file_path(stock_code, data_type)
            if not cache_file.exists():
                return False
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆé»˜è®¤7å¤©ï¼‰
        cache_info = self.metadata["stocks"][cache_key]
        cache_date = datetime.fromisoformat(cache_info["cached_date"])
        if datetime.now() - cache_date > timedelta(days=7):
            logger.info(f"ç¼“å­˜æ•°æ®å·²è¿‡æœŸ: {stock_code}")
            return False
        
        return True
    
    def get_cached_data(self, stock_code: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æ•°æ®"""
        if not self.is_data_cached(stock_code):
            return None
        
        try:
            data = {}
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            
            for data_type in data_types:
                cache_file = self.get_cache_file_path(stock_code, data_type)
                if cache_file.exists():
                    with open(cache_file, 'rb') as f:
                        data[data_type] = pickle.load(f)
            
            cache_key = self.get_cache_key(stock_code)
            cache_info = self.metadata["stocks"][cache_key]
            logger.info(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {stock_code} (ç¼“å­˜æ—¶é—´: {cache_info['cached_date']})")
            
            return data
            
        except Exception as e:
            logger.error(f"åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def cache_data(self, stock_code: str, data: Dict[str, pd.DataFrame], 
                   latest_report_date: Optional[str] = None):
        """ç¼“å­˜æ•°æ®"""
        try:
            cache_key = self.get_cache_key(stock_code)
            
            # ä¿å­˜å„ç±»æ•°æ®
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            saved_count = 0
            
            for data_type in data_types:
                if data_type in data and data[data_type] is not None:
                    cache_file = self.get_cache_file_path(stock_code, data_type)
                    with open(cache_file, 'wb') as f:
                        pickle.dump(data[data_type], f)
                    saved_count += 1
            
            # æ›´æ–°å…ƒæ•°æ®
            self.metadata["stocks"][cache_key] = {
                "stock_code": stock_code,
                "cached_date": datetime.now().isoformat(),
                "latest_report_date": latest_report_date or "",
                "data_types": [dt for dt in data_types if dt in data and data[dt] is not None],
                "file_count": saved_count
            }
            
            self._save_metadata()
            logger.info(f"æ•°æ®ç¼“å­˜æˆåŠŸ: {stock_code} (ä¿å­˜äº†{saved_count}ä¸ªæ–‡ä»¶)")
            
        except Exception as e:
            logger.error(f"ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
    
    def needs_update(self, stock_code: str, current_data: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡æ›´æ–°"""
        if not self.is_data_cached(stock_code):
            return True
        
        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_key = self.get_cache_key(stock_code)
        cache_info = self.metadata["stocks"][cache_key]
        
        # å¦‚æœå½“å‰æ•°æ®æœ‰æ›´æ–°çš„æŠ¥å‘Šæ—¥æœŸï¼Œåˆ™éœ€è¦æ›´æ–°
        if "income" in current_data and not current_data["income"].empty:
            latest_cached_date = cache_info.get("latest_report_date", "")
            
            # è·å–å½“å‰æ•°æ®çš„æœ€æ–°æŠ¥å‘Šæ—¥æœŸ
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in current_data["income"].columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in current_data["income"].columns:
                current_latest_date = pd.to_datetime(current_data["income"][date_col].iloc[0]).strftime('%Y-%m-%d')
                
                if current_latest_date > latest_cached_date:
                    logger.info(f"æ£€æµ‹åˆ°æ–°è´¢æŠ¥æ•°æ®: {stock_code} {latest_cached_date} -> {current_latest_date}")
                    return True
        
        return False
    
    def cleanup_old_cache(self, days: int = 30):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            cleaned_count = 0
            
            # æ¸…ç†è¿‡æœŸçš„è‚¡ç¥¨æ•°æ®
            expired_stocks = []
            for cache_key, cache_info in self.metadata["stocks"].items():
                cache_date = datetime.fromisoformat(cache_info["cached_date"])
                if cache_date < cutoff_date:
                    expired_stocks.append(cache_key)
            
            for cache_key in expired_stocks:
                # åˆ é™¤æ•°æ®æ–‡ä»¶
                data_types = ["income", "balance", "cashflow", "metrics", "trend"]
                for data_type in data_types:
                    cache_file = self.get_cache_file_path(cache_key.replace("stock_", ""), data_type)
                    if cache_file.exists():
                        cache_file.unlink()
                
                # ä»å…ƒæ•°æ®ä¸­åˆ é™¤
                del self.metadata["stocks"][cache_key]
                cleaned_count += 1
            
            if cleaned_count > 0:
                self._save_metadata()
                logger.info(f"æ¸…ç†äº†{cleaned_count}ä¸ªè¿‡æœŸç¼“å­˜")
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")


class AKShareFinancialDataTool(AsyncBaseToolkit):
    """AKShareè´¢åŠ¡æ•°æ®è·å–ä¸“ç”¨å·¥å…·åŒ…ç±»ï¼ˆå¸¦æ™ºèƒ½ç¼“å­˜ï¼‰"""
    
    def __init__(self, config: ToolkitConfig | dict | None = None):
        super().__init__(config)
        cache_dir = self.config.config.get("cache_dir") if self.config.config else None
        self.cache = FinancialDataCache(cache_dir)
        self.akshare: Any = None
        self._setup_akshare()
        
    def _setup_akshare(self):
        """è®¾ç½®AKShare"""
        try:
            import akshare as ak
            self.akshare = ak
            logger.info("AKShareåˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.error("AKShareæœªå®‰è£…ï¼Œæ­£åœ¨å®‰è£…...")
            import subprocess
            import sys
            subprocess.check_call([sys.executable, "-m", "pip", "install", "akshare>=1.12.0"])
            import akshare as ak
            self.akshare = ak
            logger.info("AKShareå®‰è£…æˆåŠŸ")
        except Exception as e:
            logger.error(f"AKShareè®¾ç½®å¤±è´¥: {e}")
            raise
    
    def _convert_stock_code(self, stock_code: str) -> str:
        """è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼"""
        if stock_code.startswith('6'):
            return f"SH{stock_code}"
        elif stock_code.startswith('0') or stock_code.startswith('3'):
            return f"SZ{stock_code}"
        else:
            return stock_code
    
    def _clean_financial_data(self, df: pd.DataFrame, data_type: str) -> pd.DataFrame:
        """æ¸…æ´—è´¢åŠ¡æ•°æ®"""
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯DataFrame
            if not isinstance(df, pd.DataFrame):
                logger.warning(f"è¾“å…¥æ•°æ®ä¸æ˜¯DataFrameç±»å‹: {type(df)}")
                return pd.DataFrame()
            
            # åˆ é™¤ç©ºå€¼è¡Œ
            if 'REPORT_DATE' in df.columns:
                # ä½¿ç”¨locç¡®ä¿è¿”å›DataFrameè€Œä¸æ˜¯Series
                mask = df['REPORT_DATE'].notna()
                if isinstance(mask, pd.Series):
                    df = df.loc[mask].copy()
                else:
                    df = df.copy()
            elif 'æŠ¥å‘ŠæœŸ' in df.columns:
                # ä½¿ç”¨locç¡®ä¿è¿”å›DataFrameè€Œä¸æ˜¯Series
                mask = df['æŠ¥å‘ŠæœŸ'].notna()
                if isinstance(mask, pd.Series):
                    df = df.loc[mask].copy()
                else:
                    df = df.copy()
            
            # è½¬æ¢æ•°å€¼åˆ—
            exclude_cols = ['REPORT_DATE', 'æŠ¥å‘ŠæœŸ', 'æŠ¥è¡¨ç±»å‹', 'REPORT_TYPE', 
                          'SECUCODE', 'SECURITY_CODE', 'SECURITY_NAME_ABBR']
            
            for col in df.columns:
                if col not in exclude_cols:
                    # ç¡®ä¿è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN
                    df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')
            
            # è½¬æ¢æ—¥æœŸåˆ—å¹¶æ’åº
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in df.columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in df.columns:
                df.loc[:, date_col] = pd.to_datetime(df[date_col])
                df = df.sort_values(date_col, ascending=False)
            
            logger.info(f"{data_type}æ•°æ®æ¸…æ´—å®Œæˆï¼Œå‰©ä½™{len(df)}è¡Œ")
            return df
            
        except Exception as e:
            logger.error(f"{data_type}æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
            # è¿”å›ç©ºçš„DataFrameè€Œä¸æ˜¯åŸå§‹df
            return pd.DataFrame()
    
    @register_tool()
    def get_financial_reports(self, stock_code: str, stock_name: Optional[str] = None, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
        """
        è·å–å®Œæ•´è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼ˆå¸¦æ™ºèƒ½ç¼“å­˜ï¼‰
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '600248'
            stock_name: è‚¡ç¥¨åç§°ï¼Œå¯é€‰
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
            
        Returns:
            åŒ…å«åˆ©æ¶¦è¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ã€ç°é‡‘æµé‡è¡¨çš„å­—å…¸
        """
        if stock_name is None:
            stock_name = f"è‚¡ç¥¨{stock_code}"
            
        logger.info(f"å¼€å§‹è·å–{stock_name}({stock_code})è´¢åŠ¡æ•°æ®")
        
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh:
            cached_data = self.cache.get_cached_data(stock_code)
            if cached_data:
                logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {stock_code}")
                return cached_data
        
        symbol = self._convert_stock_code(stock_code)
        logger.info(f"è‚¡ç¥¨ä»£ç è½¬æ¢: {stock_code} -> {symbol}")
        
        results: Dict[str, pd.DataFrame] = {}
        
        # ç¡®ä¿akshareå·²åˆå§‹åŒ–
        if self.akshare is None:
            logger.error("AKShareæœªæ­£ç¡®åˆå§‹åŒ–")
            return results
        
        try:
            # 1. è·å–åˆ©æ¶¦è¡¨
            logger.info("è·å–åˆ©æ¶¦è¡¨...")
            try:
                income_df = self.akshare.stock_profit_sheet_by_report_em(symbol=symbol)
                income_df = self._clean_financial_data(income_df, "åˆ©æ¶¦è¡¨")
                results['income'] = income_df
                logger.info(f"åˆ©æ¶¦è¡¨è·å–æˆåŠŸ: {len(income_df)}è¡Œ")
            except Exception as e:
                logger.error(f"åˆ©æ¶¦è¡¨è·å–å¤±è´¥: {e}")
                # å°è¯•å¤‡ç”¨æ–¹æ³•
                try:
                    income_df = self.akshare.stock_lrb_em(symbol=symbol)  # ä½¿ç”¨symbolè€Œä¸æ˜¯stock_code
                    income_df = self._clean_financial_data(income_df, "åˆ©æ¶¦è¡¨(å¤‡ç”¨)")
                    results['income'] = income_df
                    logger.info(f"åˆ©æ¶¦è¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(income_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"åˆ©æ¶¦è¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # 2. è·å–èµ„äº§è´Ÿå€ºè¡¨
            logger.info("è·å–èµ„äº§è´Ÿå€ºè¡¨...")
            try:
                balance_df = self.akshare.stock_balance_sheet_by_report_em(symbol=symbol)
                balance_df = self._clean_financial_data(balance_df, "èµ„äº§è´Ÿå€ºè¡¨")
                results['balance'] = balance_df
                logger.info(f"èµ„äº§è´Ÿå€ºè¡¨è·å–æˆåŠŸ: {len(balance_df)}è¡Œ")
            except Exception as e:
                logger.error(f"èµ„äº§è´Ÿå€ºè¡¨è·å–å¤±è´¥: {e}")
                try:
                    balance_df = self.akshare.stock_zcfz_em(symbol=symbol)  # ä½¿ç”¨symbolè€Œä¸æ˜¯stock_code
                    balance_df = self._clean_financial_data(balance_df, "èµ„äº§è´Ÿå€ºè¡¨(å¤‡ç”¨)")
                    results['balance'] = balance_df
                    logger.info(f"èµ„äº§è´Ÿå€ºè¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(balance_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"èµ„äº§è´Ÿå€ºè¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # 3. è·å–ç°é‡‘æµé‡è¡¨
            logger.info("è·å–ç°é‡‘æµé‡è¡¨...")
            try:
                cashflow_df = self.akshare.stock_cash_flow_sheet_by_report_em(symbol=symbol)
                cashflow_df = self._clean_financial_data(cashflow_df, "ç°é‡‘æµé‡è¡¨")
                results['cashflow'] = cashflow_df
                logger.info(f"ç°é‡‘æµé‡è¡¨è·å–æˆåŠŸ: {len(cashflow_df)}è¡Œ")
            except Exception as e:
                logger.error(f"ç°é‡‘æµé‡è¡¨è·å–å¤±è´¥: {e}")
                try:
                    cashflow_df = self.akshare.stock_xjll_em(symbol=symbol)  # ä½¿ç”¨symbolè€Œä¸æ˜¯stock_code
                    cashflow_df = self._clean_financial_data(cashflow_df, "ç°é‡‘æµé‡è¡¨(å¤‡ç”¨)")
                    results['cashflow'] = cashflow_df
                    logger.info(f"ç°é‡‘æµé‡è¡¨(å¤‡ç”¨æ–¹æ³•)è·å–æˆåŠŸ: {len(cashflow_df)}è¡Œ")
                except Exception as e2:
                    logger.error(f"ç°é‡‘æµé‡è¡¨å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜
            if not force_refresh and self.cache.needs_update(stock_code, results):
                logger.info(f"æ£€æµ‹åˆ°æ–°æ•°æ®ï¼Œæ›´æ–°ç¼“å­˜: {stock_code}")
            
            # è®¡ç®—å¹¶ç¼“å­˜æŒ‡æ ‡å’Œè¶‹åŠ¿æ•°æ®
            if results:
                metrics = self.get_key_metrics(results)
                trend = self.get_historical_trend(results)
                
                results['metrics'] = pd.DataFrame([metrics]) if metrics else pd.DataFrame()
                results['trend'] = trend
                
                # è·å–æœ€æ–°æŠ¥å‘Šæ—¥æœŸ
                latest_report_date = ""
                if 'income' in results and not results['income'].empty:
                    date_col = 'REPORT_DATE' if 'REPORT_DATE' in results['income'].columns else 'æŠ¥å‘ŠæœŸ'
                    if date_col in results['income'].columns:
                        latest_report_date = pd.to_datetime(results['income'][date_col].iloc[0]).strftime('%Y-%m-%d')
                
                # ç¼“å­˜æ•°æ®
                self.cache.cache_data(stock_code, results, latest_report_date)
                logger.info(f"æ•°æ®å·²ç¼“å­˜: {stock_code}")
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–è´¢åŠ¡æŠ¥è¡¨å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return results
    
    def get_key_metrics(self, financial_data: Dict[str, pd.DataFrame]) -> Dict[str, float]:
        """
        æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            
        Returns:
            å…³é”®è´¢åŠ¡æŒ‡æ ‡å­—å…¸
        """
        # ä¿ç•™åŸæ¥çš„æ–¹æ³•å®ç°
        logger.info("æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡")
        
        metrics = {}
        
        try:
            if not financial_data:
                logger.error("è´¢åŠ¡æ•°æ®ä¸ºç©º")
                return metrics
                
            # è·å–æœ€æ–°ä¸€æœŸæ•°æ®
            latest_income = financial_data.get('income', pd.DataFrame())
            latest_balance = financial_data.get('balance', pd.DataFrame())
            latest_cashflow = financial_data.get('cashflow', pd.DataFrame())
            
            if latest_income.empty:
                logger.error("åˆ©æ¶¦è¡¨æ•°æ®ä¸ºç©º")
                return metrics
                
            latest_income = latest_income.iloc[0]
            
            # æå–ç›ˆåˆ©èƒ½åŠ›æŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
            revenue_cols = ['TOTAL_OPERATE_INCOME', 'è¥ä¸šæ”¶å…¥', 'operating_revenue']
            net_profit_cols = ['NETPROFIT', 'å‡€åˆ©æ¶¦', 'net_profit']
            parent_profit_cols = ['PARENT_NETPROFIT', 'å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦', 'parent_net_profit']
            
            revenue = self._get_value_by_col_names(latest_income, revenue_cols) / 1e8
            net_profit = self._get_value_by_col_names(latest_income, net_profit_cols) / 1e8
            parent_profit = self._get_value_by_col_names(latest_income, parent_profit_cols) / 1e8
            
            metrics['revenue_billion'] = revenue
            metrics['net_profit_billion'] = net_profit
            metrics['parent_profit_billion'] = parent_profit
            metrics['net_profit_margin'] = (net_profit / revenue * 100) if revenue > 0 else 0
            
            # æå–è´¢åŠ¡çŠ¶å†µæŒ‡æ ‡
            if not latest_balance.empty:
                latest_balance = latest_balance.iloc[0]
                
                asset_cols = ['TOTAL_ASSETS', 'èµ„äº§æ€»è®¡', 'total_assets']
                liability_cols = ['TOTAL_LIABILITIES', 'è´Ÿå€ºåˆè®¡', 'total_liabilities']
                equity_cols = ['TOTAL_EQUITY', 'æ‰€æœ‰è€…æƒç›Šåˆè®¡', 'total_equity']
                
                total_assets = self._get_value_by_col_names(latest_balance, asset_cols) / 1e8
                total_liabilities = self._get_value_by_col_names(latest_balance, liability_cols) / 1e8
                total_equity = self._get_value_by_col_names(latest_balance, equity_cols) / 1e8
                
                metrics['total_assets_billion'] = total_assets
                metrics['total_liabilities_billion'] = total_liabilities
                metrics['total_equity_billion'] = total_equity
                metrics['debt_to_asset_ratio'] = (total_liabilities / total_assets * 100) if total_assets > 0 else 0
                
                # è®¡ç®—ROE
                if total_equity > 0:
                    metrics['roe'] = parent_profit / total_equity * 100
            
            logger.info("å…³é”®æŒ‡æ ‡æå–å®Œæˆ")
            return metrics
            
        except Exception as e:
            logger.error(f"æå–å…³é”®æŒ‡æ ‡å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return metrics
    
    @register_tool("get_key_metrics_from_reports")
    def get_key_metrics_from_reports(self, stock_code: str, stock_name: Optional[str] = None) -> Dict[str, float]:
        """
        æå–å…³é”®è´¢åŠ¡æŒ‡æ ‡
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            å…³é”®è´¢åŠ¡æŒ‡æ ‡å­—å…¸
        """
        # å…ˆè·å–è´¢åŠ¡æ•°æ®
        financial_data = self.get_financial_reports(stock_code, stock_name)
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•å¤„ç†
        return self.get_key_metrics(financial_data)
    
    def _get_value_by_col_names(self, row: pd.Series, col_names: List[str]) -> float:
        """æ ¹æ®å¯èƒ½çš„åˆ—åè·å–æ•°å€¼"""
        for col in col_names:
            # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNaN
            if col in row.index:
                value = row[col]
                # æ£€æŸ¥å€¼æ˜¯å¦ä¸ºNaNï¼Œä½¿ç”¨å®‰å…¨çš„æ–¹å¼æ£€æŸ¥
                try:
                    # å¯¹äºæ ‡é‡æ•°å€¼ç±»å‹
                    if isinstance(value, (int, float, np.integer, np.floating)):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºNaN
                        if not (isinstance(value, float) and np.isnan(value)):
                            return float(value)
                    elif isinstance(value, pd.Series):
                        # å¯¹äºpandas Serieså¯¹è±¡ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç©º
                        if len(value) > 0 and not pd.isna(value).all():  # ç¡®ä¿ä¸ä¸ºç©ºä¸”ä¸å…¨ä¸ºNaN
                            # è·å–ç¬¬ä¸€ä¸ªéNaNå€¼
                            valid_values = value.dropna()
                            if len(valid_values) > 0:
                                val = valid_values.iloc[0]
                                # æ£€æŸ¥æ˜¯å¦æœ‰itemæ–¹æ³•ä¸”å¯è°ƒç”¨ï¼ˆä»…å¯¹numpyæ ‡é‡ï¼‰
                                if hasattr(val, 'item') and callable(getattr(val, 'item', None)) and isinstance(val, (np.integer, np.floating)):
                                    return float(val.item())
                                else:
                                    return float(val)
                    elif isinstance(value, pd.DataFrame):
                        # DataFrameæƒ…å†µ
                        if len(value) > 0 and len(value.columns) > 0:
                            # è·å–ç¬¬ä¸€ä¸ªå€¼
                            val = value.iloc[0, 0]
                            # æ£€æŸ¥æ˜¯å¦æœ‰itemæ–¹æ³•ä¸”å¯è°ƒç”¨ï¼ˆä»…å¯¹numpyæ ‡é‡ï¼‰
                            if hasattr(val, 'item') and callable(getattr(val, 'item', None)) and isinstance(val, (np.integer, np.floating)):
                                return float(val.item())
                            else:
                                return float(val)
                    else:
                        # å…¶ä»–ç±»å‹ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                        if not (isinstance(value, float) and np.isnan(value)):
                            # æ£€æŸ¥æ˜¯å¦æœ‰itemæ–¹æ³•ä¸”å¯è°ƒç”¨ï¼ˆä»…å¯¹numpyæ ‡é‡ï¼‰
                            if hasattr(value, 'item') and callable(getattr(value, 'item', None)) and isinstance(value, (np.integer, np.floating)):
                                return float(value.item())
                            else:
                                return float(value)
                except (TypeError, ValueError, AttributeError, IndexError):
                    # å¦‚æœæ£€æŸ¥æ—¶å‡ºé”™ï¼Œå°è¯•ç›´æ¥è½¬æ¢
                    try:
                        # å¯¹äºpandaså¯¹è±¡ï¼Œå…ˆæå–å€¼å†è½¬æ¢
                        if hasattr(value, 'iloc') and not isinstance(value, (int, float, np.integer, np.floating)):
                            # ç¡®ä¿valueæœ‰é•¿åº¦å±æ€§ä¸”ä¸æ˜¯numpyæ ‡é‡
                            if hasattr(value, '__len__') and len(value) > 0 and not np.isscalar(value):
                                if isinstance(value, pd.Series):
                                    val = value.iloc[0]
                                elif isinstance(value, pd.DataFrame) and len(value.columns) > 0:
                                    val = value.iloc[0, 0]
                                else:
                                    # å¯¹äºnumpyæ•°ç»„ç­‰ï¼Œå®‰å…¨è®¿é—®
                                    try:
                                        val = value[0] if len(value) > 0 else value.item() if hasattr(value, 'item') and callable(getattr(value, 'item', None)) and isinstance(value, (np.integer, np.floating)) else value
                                    except (IndexError, TypeError):
                                        val = value.item() if hasattr(value, 'item') and callable(getattr(value, 'item', None)) and isinstance(value, (np.integer, np.floating)) else value
                                # ç¡®ä¿valä¸æ˜¯DataFrame
                                if not isinstance(val, pd.DataFrame):
                                    return float(val)
                        # æ£€æŸ¥æ˜¯å¦æœ‰itemæ–¹æ³•ä¸”å¯è°ƒç”¨ï¼Œä¸”ä¸æ˜¯åŸºæœ¬æ•°å€¼ç±»å‹ï¼ˆä»…å¯¹numpyæ ‡é‡ï¼‰
                        elif hasattr(value, 'item') and callable(getattr(value, 'item', None)) and isinstance(value, (np.integer, np.floating)):
                            return float(value.item())
                        else:
                            # æœ€åå°è¯•ç›´æ¥è½¬æ¢ï¼Œä½†æ’é™¤DataFrame
                            if not isinstance(value, pd.DataFrame):  # DataFrameä¸èƒ½ç›´æ¥è½¬æ¢ä¸ºfloat
                                return float(value)
                    except (TypeError, ValueError, IndexError):
                        continue
        return 0.0
    
    def get_historical_trend(self, financial_data: Dict[str, pd.DataFrame], years: int = 4) -> pd.DataFrame:
        """
        è·å–å†å²è¶‹åŠ¿æ•°æ®ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            years: è·å–å¹´æ•°
            
        Returns:
            è¶‹åŠ¿æ•°æ®DataFrame
        """
        # ä¿ç•™åŸæ¥çš„æ–¹æ³•å®ç°
        logger.info(f"è·å–æœ€è¿‘{years}å¹´è¶‹åŠ¿æ•°æ®")
        
        try:
            income_df = financial_data.get('income', pd.DataFrame())
            if income_df.empty:
                logger.error("åˆ©æ¶¦è¡¨æ•°æ®ä¸ºç©º")
                return pd.DataFrame()
            
            # è·å–æœ€è¿‘Nå¹´æ•°æ®
            trend_data = income_df.head(years).copy()
            
            # ç¡®å®šæ—¥æœŸåˆ—å
            date_col = 'REPORT_DATE' if 'REPORT_DATE' in trend_data.columns else 'æŠ¥å‘ŠæœŸ'
            if date_col in trend_data.columns:
                trend_data.loc[:, 'å¹´ä»½'] = pd.to_datetime(trend_data[date_col]).dt.year
            else:
                logger.error("æœªæ‰¾åˆ°æ—¥æœŸåˆ—")
                return pd.DataFrame()
            
            # æå–å…³é”®æŒ‡æ ‡ï¼ˆæ”¯æŒå¤šç§åˆ—åï¼‰
            revenue_cols = ['TOTAL_OPERATE_INCOME', 'è¥ä¸šæ”¶å…¥', 'operating_revenue']
            profit_cols = ['NETPROFIT', 'å‡€åˆ©æ¶¦', 'net_profit']
            
            result_df = pd.DataFrame()
            # ç¡®ä¿ä»trend_dataä¸­æå–çš„æ˜¯Series
            if 'å¹´ä»½' in trend_data.columns:
                result_df['å¹´ä»½'] = trend_data['å¹´ä»½'].copy()
            result_df['è¥ä¸šæ”¶å…¥'] = self._get_values_by_col_names(trend_data, revenue_cols) / 1e8
            result_df['å‡€åˆ©æ¶¦'] = self._get_values_by_col_names(trend_data, profit_cols) / 1e8
            
            logger.info(f"è¶‹åŠ¿æ•°æ®è·å–æˆåŠŸ: {len(result_df)}å¹´")
            return result_df
            
        except Exception as e:
            logger.error(f"è·å–è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    @register_tool("get_historical_trend_from_reports")
    def get_historical_trend_from_reports(self, stock_code: str, stock_name: Optional[str] = None, years: int = 4) -> str:
        """
        è·å–å†å²è¶‹åŠ¿æ•°æ®
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            years: è·å–å¹´æ•°
            
        Returns:
            è¶‹åŠ¿æ•°æ®çš„JSONå­—ç¬¦ä¸²
        """
        # å…ˆè·å–è´¢åŠ¡æ•°æ®
        financial_data = self.get_financial_reports(stock_code, stock_name)
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•å¤„ç†
        trend_df = self.get_historical_trend(financial_data, years)
        # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²è¿”å›
        json_str = trend_df.to_json(orient='records', date_format='iso')
        return json_str if json_str is not None else "[]"
    
    def _get_values_by_col_names(self, df: pd.DataFrame, col_names: List[str]) -> pd.Series:
        """æ ¹æ®å¯èƒ½çš„åˆ—åè·å–æ•°å€¼åˆ—"""
        for col in col_names:
            if col in df.columns:
                series = df[col]
                # ç¡®ä¿è¿”å›çš„æ˜¯Seriesç±»å‹
                if isinstance(series, pd.Series):
                    return series.copy()
                else:
                    # å¦‚æœä¸æ˜¯Seriesï¼Œåˆ›å»ºä¸€ä¸ªSeries
                    return pd.Series([series], index=[0])
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åˆ—ï¼Œè¿”å›é›¶å€¼Series
        return pd.Series([0.0] * len(df), index=df.index) if len(df) > 0 else pd.Series([0.0])
    
    def save_to_csv(self, financial_data: Dict[str, pd.DataFrame], filepath_prefix: str):
        """
        ä¿å­˜è´¢åŠ¡æ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        
        Args:
            financial_data: è´¢åŠ¡æ•°æ®å­—å…¸
            filepath_prefix: æ–‡ä»¶è·¯å¾„å‰ç¼€
        """
        # ä¿ç•™åŸæ¥çš„æ–¹æ³•å®ç°
        logger.info(f"ä¿å­˜è´¢åŠ¡æ•°æ®åˆ° {filepath_prefix}")
        
        try:
            for data_type, df in financial_data.items():
                if not df.empty:
                    filepath = f"{filepath_prefix}_{data_type}.csv"
                    df.to_csv(filepath, index=False, encoding='utf-8-sig')
                    logger.info(f"{data_type}æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")
                    
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
    
    @register_tool("save_financial_data")
    def save_financial_data(self, stock_code: str, stock_name: Optional[str] = None, filepath_prefix: str = "financial_data"):
        """
        ä¿å­˜è´¢åŠ¡æ•°æ®åˆ°CSVæ–‡ä»¶
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            filepath_prefix: æ–‡ä»¶è·¯å¾„å‰ç¼€
        """
        # å…ˆè·å–è´¢åŠ¡æ•°æ®
        financial_data = self.get_financial_reports(stock_code, stock_name)
        # è°ƒç”¨å†…éƒ¨æ–¹æ³•å¤„ç†
        self.save_to_csv(financial_data, filepath_prefix)
    
    @register_tool()
    def check_cache_status(self, stock_code: str) -> Dict:
        """
        æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        """
        cache_key = self.cache.get_cache_key(stock_code)
        
        if cache_key in self.cache.metadata["stocks"]:
            cache_info = self.cache.metadata["stocks"][cache_key]
            return {
                "cached": True,
                "cached_date": cache_info["cached_date"],
                "latest_report_date": cache_info.get("latest_report_date", ""),
                "data_types": cache_info.get("data_types", []),
                "file_count": cache_info.get("file_count", 0)
            }
        else:
            return {
                "cached": False,
                "cached_date": "",
                "latest_report_date": "",
                "data_types": [],
                "file_count": 0
            }
    
    @register_tool()
    def refresh_cache(self, stock_code: str, stock_name: Optional[str] = None) -> bool:
        """
        å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            stock_name: è‚¡ç¥¨åç§°
            
        Returns:
            åˆ·æ–°æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"å¼ºåˆ¶åˆ·æ–°ç¼“å­˜: {stock_code}")
        
        try:
            # å¼ºåˆ¶è·å–æ–°æ•°æ®
            new_data = self.get_financial_reports(stock_code, stock_name, force_refresh=True)
            
            if new_data and ('income' in new_data and not new_data['income'].empty):
                logger.info(f"ç¼“å­˜åˆ·æ–°æˆåŠŸ: {stock_code}")
                return True
            else:
                logger.warning(f"ç¼“å­˜åˆ·æ–°å¤±è´¥ï¼Œæ•°æ®ä¸ºç©º: {stock_code}")
                return False
                
        except Exception as e:
            logger.error(f"ç¼“å­˜åˆ·æ–°å¤±è´¥: {e}")
            return False
    
    @register_tool()
    def cleanup_cache(self, days: int = 30):
        """
        æ¸…ç†è¿‡æœŸç¼“å­˜
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        logger.info(f"æ¸…ç†{days}å¤©å‰çš„ç¼“å­˜")
        self.cache.cleanup_old_cache(days)
    
    @register_tool()
    def get_cache_info(self) -> Dict:
        """
        è·å–ç¼“å­˜æ•´ä½“ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        stocks = self.cache.metadata["stocks"]
        total_files = sum(info.get("file_count", 0) for info in stocks.values())
        
        # è®¡ç®—ç¼“å­˜å¤§å°
        total_size = 0
        for stock_code in stocks.keys():
            data_types = ["income", "balance", "cashflow", "metrics", "trend"]
            for data_type in data_types:
                cache_file = self.cache.get_cache_file_path(stock_code.replace("stock_", ""), data_type)
                if cache_file.exists():
                    total_size += cache_file.stat().st_size
        
        return {
            "total_stocks": len(stocks),
            "total_files": total_files,
            "total_size_mb": round(total_size / 1024 / 1024, 2),
            "last_cleanup": self.cache.metadata.get("last_cleanup", ""),
            "cache_directory": str(self.cache.cache_dir)
        }
    
    @register_tool()
    def clear_all_cache(self):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
        logger.warning("æ¸…é™¤æ‰€æœ‰ç¼“å­˜æ•°æ®")
        
        try:
            # åˆ é™¤æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            for stock_info in list(self.cache.metadata["stocks"].values()):
                stock_code = stock_info["stock_code"]
                data_types = ["income", "balance", "cashflow", "metrics", "trend"]
                for data_type in data_types:
                    cache_file = self.cache.get_cache_file_path(stock_code, data_type)
                    if cache_file.exists():
                        cache_file.unlink()
            
            # é‡ç½®å…ƒæ•°æ®
            self.cache.metadata = {
                "stocks": {},
                "last_cleanup": datetime.now().isoformat(),
                "version": "1.0"
            }
            self.cache._save_metadata()
            
            logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…é™¤")
            
        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    @register_tool()
    def export_cache_summary(self, filepath: Optional[str] = None):
        """
        å¯¼å‡ºç¼“å­˜æ‘˜è¦
        
        Args:
            filepath: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if filepath is None:
            filepath = str(self.cache.cache_dir / "cache_summary.csv")
        
        try:
            summary_data = []
            for cache_key, info in self.cache.metadata["stocks"].items():
                summary_data.append({
                    "stock_code": info["stock_code"],
                    "cached_date": info["cached_date"],
                    "latest_report_date": info.get("latest_report_date", ""),
                    "data_types": ", ".join(info.get("data_types", [])),
                    "file_count": info.get("file_count", 0)
                })
            
            df = pd.DataFrame(summary_data)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            logger.info(f"ç¼“å­˜æ‘˜è¦å·²å¯¼å‡ºåˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºç¼“å­˜æ‘˜è¦å¤±è´¥: {e}")


# ä¾¿åˆ©å‡½æ•°
def get_financial_reports(stock_code: str, stock_name: Optional[str] = None, force_refresh: bool = False) -> Dict[str, pd.DataFrame]:
    """è·å–è´¢åŠ¡æŠ¥è¡¨æ•°æ®çš„ä¾¿åˆ©å‡½æ•°"""
    tool = AKShareFinancialDataTool()
    return tool.get_financial_reports(stock_code, stock_name, force_refresh)

def get_key_metrics(financial_data: Dict[str, pd.DataFrame]) -> Dict[str, float]:
    """è·å–å…³é”®è´¢åŠ¡æŒ‡æ ‡çš„ä¾¿åˆ©å‡½æ•°"""
    tool = AKShareFinancialDataTool()
    return tool.get_key_metrics(financial_data)

def get_historical_trend(financial_data: Dict[str, pd.DataFrame], years: int = 4) -> pd.DataFrame:
    """è·å–å†å²è¶‹åŠ¿æ•°æ®çš„ä¾¿åˆ©å‡½æ•°"""
    tool = AKShareFinancialDataTool()
    return tool.get_historical_trend(financial_data, years)

# ç¼“å­˜ç®¡ç†ä¾¿åˆ©å‡½æ•°
def check_cache_status(stock_code: str) -> Dict:
    """æ£€æŸ¥ç¼“å­˜çŠ¶æ€"""
    tool = AKShareFinancialDataTool()
    return tool.check_cache_status(stock_code)

def refresh_cache(stock_code: str, stock_name: Optional[str] = None) -> bool:
    """åˆ·æ–°ç¼“å­˜"""
    tool = AKShareFinancialDataTool()
    return tool.refresh_cache(stock_code, stock_name)

def cleanup_cache(days: int = 30):
    """æ¸…ç†è¿‡æœŸç¼“å­˜"""
    tool = AKShareFinancialDataTool()
    tool.cleanup_cache(days)

def get_cache_info() -> Dict:
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    tool = AKShareFinancialDataTool()
    return tool.get_cache_info()

def clear_all_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜"""
    tool = AKShareFinancialDataTool()
    tool.clear_all_cache()

def export_cache_summary(filepath: Optional[str] = None):
    """å¯¼å‡ºç¼“å­˜æ‘˜è¦"""
    tool = AKShareFinancialDataTool()
    tool.export_cache_summary(filepath)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== AKShareè´¢åŠ¡æ•°æ®å·¥å…·å®Œæ•´æµ‹è¯•ï¼ˆå«æ™ºèƒ½ç¼“å­˜ï¼‰ ===\n")
    
    # åˆå§‹åŒ–å·¥å…·
    tool = AKShareFinancialDataTool()
    
    # æµ‹è¯•1: é¦–æ¬¡è·å–æ•°æ®ï¼ˆåº”è¯¥ä»ç½‘ç»œè·å–ï¼‰
    print("1. é¦–æ¬¡è·å–é™•è¥¿å»ºå·¥æ•°æ®ï¼ˆä»ç½‘ç»œè·å–ï¼‰...")
    data1 = tool.get_financial_reports("600248", "é™•è¥¿å»ºå·¥")
    
    if data1:
        print("   âœ“ æ•°æ®è·å–æˆåŠŸ")
        
        # æ£€æŸ¥ç¼“å­˜çŠ¶æ€
        cache_status = tool.check_cache_status("600248")
        print(f"   âœ“ ç¼“å­˜çŠ¶æ€: {cache_status}")
        
        # æµ‹è¯•å…³é”®æŒ‡æ ‡
        metrics = tool.get_key_metrics(data1)
        if metrics:
            print(f"   âœ“ å…³é”®æŒ‡æ ‡: è¥æ”¶ {metrics.get('revenue_billion', 0):.1f}äº¿å…ƒ")
    
    # æµ‹è¯•2: ç¬¬äºŒæ¬¡è·å–æ•°æ®ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
    print("\n2. ç¬¬äºŒæ¬¡è·å–é™•è¥¿å»ºå·¥æ•°æ®ï¼ˆä»ç¼“å­˜è·å–ï¼‰...")
    data2 = tool.get_financial_reports("600248", "é™•è¥¿å»ºå·¥")
    
    if data2:
        print("   âœ“ ç¼“å­˜æ•°æ®è·å–æˆåŠŸ")
        
        # æ¯”è¾ƒä¸¤æ¬¡æ•°æ®æ˜¯å¦ä¸€è‡´
        if data1 and data2:
            income_match = len(data1.get('income', pd.DataFrame())) == len(data2.get('income', pd.DataFrame()))
            print(f"   âœ“ æ•°æ®ä¸€è‡´æ€§: {'é€šè¿‡' if income_match else 'å¤±è´¥'}")
    
    # æµ‹è¯•3: ç¼“å­˜ç®¡ç†åŠŸèƒ½
    print("\n3. æµ‹è¯•ç¼“å­˜ç®¡ç†åŠŸèƒ½...")
    
    # è·å–ç¼“å­˜ä¿¡æ¯
    cache_info = tool.get_cache_info()
    print(f"   âœ“ ç¼“å­˜ç»Ÿè®¡: {cache_info['total_stocks']}åªè‚¡ç¥¨, {cache_info['total_size_mb']}MB")
    
    # æµ‹è¯•å¼ºåˆ¶åˆ·æ–°
    refresh_success = tool.refresh_cache("600248", "é™•è¥¿å»ºå·¥")
    print(f"   âœ“ å¼ºåˆ¶åˆ·æ–°: {'æˆåŠŸ' if refresh_success else 'å¤±è´¥'}")
    
    # æµ‹è¯•4: å¤šè‚¡ç¥¨ç¼“å­˜
    print("\n4. æµ‹è¯•å¤šè‚¡ç¥¨ç¼“å­˜...")
    test_stocks = [("000858", "äº”ç²®æ¶²"), ("600519", "è´µå·èŒ…å°")]
    
    for code, name in test_stocks:
        print(f"   è·å– {name}({code})...")
        try:
            data = tool.get_financial_reports(code, name)
            if data and 'income' in data and not data['income'].empty:
                metrics = tool.get_key_metrics(data)
                print(f"     âœ“ æˆåŠŸ - è¥æ”¶: {metrics.get('revenue_billion', 0):.1f}äº¿å…ƒ")
            else:
                print(f"     âœ— å¤±è´¥")
        except Exception as e:
            print(f"     âœ— å¼‚å¸¸: {e}")
    
    # æµ‹è¯•5: æœ€ç»ˆç¼“å­˜ç»Ÿè®¡
    print("\n5. æœ€ç»ˆç¼“å­˜ç»Ÿè®¡...")
    final_cache_info = tool.get_cache_info()
    print(f"   æ€»ç¼“å­˜è‚¡ç¥¨æ•°: {final_cache_info['total_stocks']}")
    print(f"   æ€»ç¼“å­˜æ–‡ä»¶æ•°: {final_cache_info['total_files']}")
    print(f"   æ€»ç¼“å­˜å¤§å°: {final_cache_info['total_size_mb']}MB")
    print(f"   ç¼“å­˜ç›®å½•: {final_cache_info['cache_directory']}")
    
    # å¯¼å‡ºç¼“å­˜æ‘˜è¦
    tool.export_cache_summary()
    print("   âœ“ ç¼“å­˜æ‘˜è¦å·²å¯¼å‡º")
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print("âœ“ æ™ºèƒ½ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
    print("âœ“ å¢é‡æ›´æ–°æœºåˆ¶å·¥ä½œ")
    print("âœ“ ç¼“å­˜ç®¡ç†åŠŸèƒ½å®Œæ•´")
    print("âœ“ æ•°æ®ä¸€è‡´æ€§ä¿è¯")
    print("\nğŸ‰ å·¥å…·å‡çº§å®Œæˆï¼ç°åœ¨å…·å¤‡æ™ºèƒ½ç¼“å­˜å’Œå¢é‡æ›´æ–°åŠŸèƒ½ã€‚")
    print("\nä¸»è¦ç‰¹æ€§:")
    print("- åŒä¸€å…¬å¸æ•°æ®è·å–ä¸€æ¬¡åè‡ªåŠ¨ç¼“å­˜")
    print("- æ£€æµ‹æ–°è´¢æŠ¥æ•°æ®å¹¶è‡ªåŠ¨å¢é‡æ›´æ–°")
    print("- ç¼“å­˜æœ‰æ•ˆæœŸç®¡ç†ï¼ˆé»˜è®¤7å¤©ï¼‰")
    print("- è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜")
    print("- å®Œæ•´çš„ç¼“å­˜ç®¡ç†åŠŸèƒ½")